{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba2f0f1",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f14957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     -------------- ------------------------ 41.0/107.3 kB 2.0 MB/s eta 0:00:01\n",
      "     ----------------- ------------------- 51.2/107.3 kB 871.5 kB/s eta 0:00:01\n",
      "     ----------------- ------------------- 51.2/107.3 kB 871.5 kB/s eta 0:00:01\n",
      "     ---------------------------- -------- 81.9/107.3 kB 512.0 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 102.4/107.3 kB 590.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 107.3/107.3 kB 443.0 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\rutik\\anaconda\\envs\\ruth\\lib\\site-packages (from tflearn) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\rutik\\anaconda\\envs\\ruth\\lib\\site-packages (from tflearn) (1.16.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\rutik\\anaconda\\envs\\ruth\\lib\\site-packages (from tflearn) (10.3.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py): started\n",
      "  Building wheel for tflearn (setup.py): finished with status 'done'\n",
      "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127292 sha256=019a5c8f79fa47cfe040eaad73caa6745b9afd33d8c85ee8f707da7a918d3f30\n",
      "  Stored in directory: c:\\users\\rutik\\appdata\\local\\pip\\cache\\wheels\\2e\\5f\\cd\\ffa06b65baff30574958318ce36ef9ef7af1ef3745dadaf420\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b406916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rutik\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tflearn\\__init__.py:5: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\rutik\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\rutik\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (C:\\Users\\rutik\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Used in Tensorflow Model \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtflearn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n",
      "File \u001b[1;32m~\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tflearn\\__init__.py:25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize, summarize_activations, \\\n\u001b[0;32m     22\u001b[0m     summarize_gradients, summarize_variables, summarize_all\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Predefined ops\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalization\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n",
      "File \u001b[1;32m~\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tflearn\\layers\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m batch_normalization, local_response_normalization\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m regression\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lstm, gru, simple_rnn, bidirectional_rnn, \\\n\u001b[0;32m     12\u001b[0m     BasicRNNCell, BasicLSTMCell, GRUCell\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge, merge_outputs\n",
      "File \u001b[1;32m~\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tflearn\\layers\\recurrent.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rnn_cell_impl \u001b[38;5;28;01mas\u001b[39;00m _rnn_cell, dynamic_rnn \u001b[38;5;28;01mas\u001b[39;00m _drnn\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core_rnn_cell\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_sequence\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'is_sequence' from 'tensorflow.python.util.nest' (C:\\Users\\rutik\\anaconda\\envs\\Ruth\\Lib\\site-packages\\tensorflow\\python\\util\\nest.py)"
     ]
    }
   ],
   "source": [
    "#Used in Tensorflow Model \n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "#Other\n",
    "import json # for intent\n",
    "import pickle # for saving model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b6aba",
   "metadata": {},
   "source": [
    "## Contextualisation and other NLP Tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1c747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96618c66",
   "metadata": {},
   "source": [
    "## Loading intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing the Intents.....\")\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910adf21",
   "metadata": {},
   "source": [
    "## Generating words, classes, documents and ignore_words from intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4523fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "print(\"Looping through the Intents to Convert them to words, classes, documents, and ignore_words....\")\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # Add to our words list\n",
    "        words.extend(w)\n",
    "        # Add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f8531",
   "metadata": {},
   "source": [
    "## Lemmatization, Lowering and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b54738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Lemmatization, Lowering.......\")\n",
    "# lemmaztize and lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338dbb11",
   "metadata": {},
   "source": [
    "## Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b363d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort classes\n",
    "classes = sorted(list(set (classes)))\n",
    "\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653c8b5",
   "metadata": {},
   "source": [
    "## Creating training set and bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fdfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating the Data for our Model....\")\n",
    "training = []\n",
    "output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating an List (Empty) for Output.....\")\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e7a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Traning Set, Bag of Words for our Model....\")\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    #List of tokenize words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c95d3",
   "metadata": {},
   "source": [
    "## Converting training set into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70236be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shuffling Randomly and Converting into Numpy Array for Faster Processing......\")\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e354fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Train and Test Lists.....\")\n",
    "train_x = list(training[:,0])\n",
    "train_y= list(training[:,1])\n",
    "print(\"Building Neural Network for Out Chatbot to be Contextual.. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resetting graph data....\")\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9fcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multi-class classification tasks.\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "# 1st number of training examples and 2nd number of input neurons\n",
    "net = tflearn.fully_connected (net, 8)\n",
    "# fully connected layers to the neural network\n",
    "net = tflearn.fully_connected (net, 8)\n",
    "net = tflearn.fully_connected (net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "# output layer with a softmax activation function\n",
    "print(\"Training....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2bbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN (net, tensorboard_dir='tflearn_logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6d661b",
   "metadata": {},
   "source": [
    "## Training model and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training the Model.......\")\n",
    "model.fit(train_x, train_y, n_epoch=500, batch_size=8, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96335cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving the Model.......\")\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pickle is also Saved........\")\n",
    "pickle.dump({'texts.pkl': words, 'labels.pkl':classes,\n",
    "             'train_x':train_x, 'train_y':train_y},\n",
    "            open(\"training_data\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81078941",
   "metadata": {},
   "source": [
    "## Loading pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8001aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Pickle.....\")\n",
    "data = pickle.load(open(\"training_data\", \"rb\" ) )\n",
    "words = data['texts.pkl']\n",
    "classes = data['labels.pkl']\n",
    "train_x = data['train_x']\n",
    "train_y= data['train_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e8f4c",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our saved model\n",
    "#loaded_model = tf.keras.models.load_model('model.tflearn')\n",
    "model.load('./model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e615512",
   "metadata": {},
   "source": [
    "## defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b337aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array.\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f86b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Array of Bag of Words: True or False and 0 or 1 for each word of bag that exists in the Sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate (words):\n",
    "            if w == s:\n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_THRESHOLD = 0.95\n",
    "print(\"ERROR_THRESHOLD = 0.95\")\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    results = [[i, r] for i, r in enumerate(res)]\n",
    "\n",
    "    # Check if any prediction is above the threshold\n",
    "    predictions_above_threshold = [r for r in results if r[1] > ERROR_THRESHOLD]\n",
    "\n",
    "    # If there are predictions above threshold, sort them by strength of probability\n",
    "    if predictions_above_threshold:\n",
    "        predictions_above_threshold.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in predictions_above_threshold]\n",
    "    else:\n",
    "        # No predictions above threshold, return \"Could not understand\" response\n",
    "        return_list = [{\"intent\": \"invalid\", \"probability\": \"N/A\"}]\n",
    "\n",
    "    return return_list\n",
    "\n",
    "# ERROR_THRESHOLD = 0.95\n",
    "# print(\"ERROR_THRESHOLD = 0.95\")\n",
    "\n",
    "# def predict_class(sentence, model):\n",
    "#     # filter out predictions below a threshold\n",
    "#     p = bow(sentence, words,show_details=False)\n",
    "#     res = model.predict(np.array([p]))[0]\n",
    "#     results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "#     # sort by strength of probability\n",
    "#     results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return_list = []\n",
    "#     for r in results:\n",
    "#         return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "#     return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5dab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "# def response(sentence,show_details=False):\n",
    "#     results = predict_class(sentence)\n",
    "#     # That Means if classification is Done then Find the Matching Tag.\n",
    "#     if results:\n",
    "#         # Long Loop to get the Result.\n",
    "#         while results:\n",
    "#             for i in intents['intents']:\n",
    "#                 #Tag Finding\n",
    "#                 if i['tag'] == results[0][0]:\n",
    "#                     # Random Response from High Order Probabilities\n",
    "#                     return print(\"Bot : \",random.choice(i['responses']))\n",
    "                \n",
    "#             results.pop(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b92ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg')\n",
    "    return chatbot_response(userText)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
